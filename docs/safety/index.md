# Safety Module

The **Safety Module** evaluates jailbreak robustness, harmfulness resistance, and the modelâ€™s ability to comply with safety constraints under adversarial input conditions.

## Key Objectives
- Measure resistance to jailbreak attacks  
- Assess refusal ability on harmful or unethical prompts  
- Evaluate toxic or unsafe output content  
- Analyze model robustness through adversarial transformations  

## Attack Methods in OmniTrust
This module includes multiple attack types, such as:

- [CipherChat](cipherchat.md)
- Optimization-based adversarial attacks

Use the navigation panel to access documentation for each method.