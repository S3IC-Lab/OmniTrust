{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#omnitrust","title":"OmniTrust","text":"<p>A Comprehensive Trustworthiness Evaluation Platform for Large Language Models (LLMs)</p> <p>OmniTrust provides unified, modular, and reproducible evaluation pipelines across six major trustworthiness modules:</p> <ul> <li>\ud83d\udd10 Safety</li> <li>\ud83d\udd75\ufe0f Privacy</li> <li>\ud83e\udde9 Detectability</li> <li>\ud83e\udde0 Hallucination</li> <li>\u2696\ufe0f Fairness</li> <li>\ud83c\udfaf Fidelity</li> </ul>"},{"location":"#explore-modules","title":"Explore Modules","text":""},{"location":"#safety-module","title":"\ud83d\udd10 Safety Module","text":"<p>Evaluate jailbreak robustness, harmful prompt resistance, and adversarial safety behavior.</p>"},{"location":"#privacy-module","title":"\ud83d\udd75\ufe0f Privacy Module","text":"<p>Assess risks such as PII leakage, membership inference, and attribute inference.</p>"},{"location":"#detectability-module","title":"\ud83e\udde9 Detectability Module","text":"<p>Analyze watermark detection, signature robustness, and model tracing.</p>"},{"location":"#hallucination-module","title":"\ud83e\udde0 Hallucination Module","text":"<p>Measure factual correctness and hallucination frequency.</p>"},{"location":"#fairness-module","title":"\u2696\ufe0f Fairness Module","text":"<p>Detect demographic bias and stereotype generation.</p>"},{"location":"#fidelity-module","title":"\ud83c\udfaf Fidelity Module","text":"<p>Evaluate consistency, stability, and reasoning coherence.</p>"},{"location":"safety/","title":"Safety Module","text":"<p>The Safety Module evaluates jailbreak robustness, harmfulness resistance, and the model\u2019s ability to comply with safety constraints under adversarial input conditions.</p>"},{"location":"safety/#key-objectives","title":"Key Objectives","text":"<ul> <li>Measure resistance to jailbreak attacks  </li> <li>Assess refusal ability on harmful or unethical prompts  </li> <li>Evaluate toxic or unsafe output content  </li> <li>Analyze model robustness through adversarial transformations  </li> </ul>"},{"location":"safety/#attack-methods-in-omnitrust","title":"Attack Methods in OmniTrust","text":"<p>This module includes multiple attack types, such as:</p> <ul> <li>Cipher-based jailbreaks  </li> <li>Optimization-based adversarial attacks</li> </ul> <p>Use the navigation panel to access documentation for each method.</p>"},{"location":"safety/cipherchat/","title":"CipherChat","text":""},{"location":"safety/cipherchat/#cipherchat","title":"CipherChat","text":"<p>Dataset: safetyprompts (Chinese safety assessment benchmark) Attack Category: Jailbreak / Adversarial Prompt Encoding Target Models: - gpt-3.5-turbo-0613 - gpt-4-0613 - text-davinci-003 - text-curie-001 - text-babbage-001  </p>"},{"location":"safety/cipherchat/#overview","title":"\ud83d\udd0d Overview","text":"<p>CipherChat is a jailbreak method that encodes harmful instructions using various cipher transformations (e.g., Morse code, substitution encoding) to bypass model safety filters. The LLM is then guided to decode and respond to the original harmful request.</p>"},{"location":"safety/cipherchat/#attack-parameters","title":"\ud83e\udde9 Attack Parameters","text":"Parameter Description <code>--attack CipherChat</code> Selects CipherChat attack method <code>--dataset</code> Dataset name (e.g., <code>safetyprompts</code>) <code>--data-dir</code> Path to dataset directory <code>--output-file</code> Save path for attack results <code>--model-path</code> Path or name of target model <code>--model_name</code> Model name string (<code>gpt-3.5-turbo-0613</code>, default) <code>--encode_method</code> Encoding type (<code>morse</code>, <code>unchange</code>, etc.) <code>--instruction_type</code> Harmful domain category (default: <code>Crimes_And_Illegal_Activities</code>) <code>--demonstration_toxicity</code> Toxicity label (<code>toxic</code>, default) <code>--language</code> Prompt language (<code>zh</code>, default)"},{"location":"safety/cipherchat/#environment-setup","title":"\ud83d\udd27 Environment Setup","text":"<p>change it into real ones...</p>"},{"location":"safety/cipherchat/#run-cipherchat","title":"\ud83d\ude80 Run CipherChat","text":"<p>Example command:</p> <pre><code>  python examples/jailbreak.py --attack CipherChat --dataset safetyprompts --data-dir ./data/dataset/safetyprompts --output-file attack/jailbreak/utils/CipherChat/saved_results --model-path /home/balei/models/vicuna/vicuna-13b-v1.5/ CipherChat --model_name gpt-3.5-turbo-0613 --encode_method morse --instruction_type Crimes_And_Illegal_Activities --demonstration_toxicity toxic --language zh\n</code></pre> <p>Note: The second <code>CipherChat</code> is used to indicate the subparser <code>cipherchat_parser = subparsers.add_parser(\"CipherChat\", help=\"Run CipherChat attack\")</code>.</p>"},{"location":"safety/cipherchat/#output","title":"\ud83d\udcc1 Output","text":"<p>Saved Results: <code>attack/jailbreak/utils/CipherChat/saved_results</code> </p>"},{"location":"safety/cipherchat/#summary","title":"\u2714 Summary","text":"<p>CipherChat is a practical and modular jailbreak attack that evaluates an LLM\u2019s ability to resist harmful encoded instructions. It supports multiple ciphers, toxic domains, and multilingual prompts.</p>"}]}